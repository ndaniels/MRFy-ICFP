\newif\ifpdfmadness

\ifx\pdfoutput\undefined
   \pdfmadnessfalse
\else
   \ifnum\pdfoutput=1
     \pdfmadnesstrue
   \else
     \pdfmadnessfalse
   \fi
\fi

\documentclass[preprint,nonatbib,blockstyle,nocopyrightspace,times]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{array}
\usepackage{listings}
\usepackage{graphicx}
\lstset{
    tabsize = 2,
    basicstyle = \ttfamily,
    language = Haskell
    }    

\newcommand\figref[1]{Figure~\ref{#1}}

\renewcommand\ttdefault{aett}

  \long\def\remark#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}

\usepackage[authoryear]{natbib}
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar


\begin{document}


\conferenceinfo{ICFP '12}{September 9-15, Copenhagen.} 
\copyrightyear{2012} 
\copyrightdata{[to be supplied]} 

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{Haskell in Computational Biology}   % 'preprint' option specified.

\title{Experience Report: Haskell in Computational Biology}
% \subtitle{Subtitle Text, if any}

\authorinfo{Noah M. Daniels \and Andrew Gallant \and Norman Ramsey}
           {Department of Computer Science, Tufts University}
           {\{ndaniels, agallant, nr\}@cs.tufts.edu}


\maketitle

\begin{abstract}
HELLO?  DRAFT OF AN ABSTRACT?
\end{abstract}

% \category{CR-number}{subcategory}{third-level}
% 
% \terms
% term1, term2
% 
% \keywords
% keyword1, keyword2

\section{Introduction}

Computational biologists write software that answers questions about 
sequences of nucleic acids (genomic data) or sequences of amino 
acids (proteomic data). 
For some software, considerations of performance are paramount; this
software is usually written C~or~C++. 
For other software, considerations of convenience, readability, and
productivity are more important;
this software is usually written in a ``scripting'' language like
Perl, Python, Ruby, or~R.\remark{Don't forget audience.}
In~this Experience Report, we show that computational biologists can
successfully replace both these families of languages with Haskell:
\begin{itemize}
\item
Our experimental Haskell programs are as easy to understand and change
 as the programs our group writes using scripting languages.
\item
The performance of our Haskell code is comparable to the performance
of~C++.
\item
We have experienced barriers to entry, of which the most salient has been
the difficulty of understanding and tuning  the performance of Haskell
programs. 
We found the other barriers 
relatively easy to overcome.
\end{itemize}
Our experience is based on MRFy (pronounced ``Murphy''), which
implements a novel family of algorithms for finding remote homologs to 
newly discovered proteins. 

\section{The biology}

Proteins are cellular machinery. They interact with one another and with other 
molecules to carry out the functions of living cells: metabolism, regulation, 
signalling, and more.
A~protein's function is determined by its structure, 
and its structure is determined by the sequence of amino acids that
form the protein.
\remark{Lost: there are only 20-odd amino acids}
The amino-acid sequence is ultimately determined by a sequence of
nucleic acids in DNA, which we call a gene.
Given a genetic sequence, biologists wish to know the cellular
function of the protein that gene codes for.
The best known method of discovering such function is
to find other proteins of 
similar structure, which likely share similar function.
This problem is called 
\textit{homology detection}.




Computational biologists detect homologies by building 
algorithms which, given an amino-acid sequence,\remark{This sequence
  (the sequence ``under test'') needs a name.  (Urk! I see below it is
  called the ``query sequence.''  This name needs to come in earlier.) ---NR}
find known proteins of
similar structure.
When the similar proteins are formed from amino-acid sequences that
are nearly identical to the sequence under test, the homologies can be
detected by  
simple string-matching algorithms.\remark{Can we just drop the
  sentence about string matching?}
When the similar proteins are formed from amino-acid sequences that
are not too far in edit distance from the sequence under test, the homologies can be
detected by
a family of algorithms called 
\textit{hidden Markov models}~\cite{hmmer}.\remark{citation needed?}
But in real biological systems,
proteins with similar structure and function may be formed from significantly 
different amino-acid sequences, which are not close in edit distance.
Our~research contribution---the MRFy algorithm---can detect homologies
in amino-acid sequences that are only distantly related.
%
%We will explain an 
%algorithm for detecting reasonably similar sequences, and then move on to 
%explain MRFy, our novel approach for detecting more distantly related sequences 
%for proteins that share similar structure and function.

\emph{[We need an argument here saying why an understanding of the
    algorithms is relevant to the claims made in the experience
    report, then say how we present the algorithms, then say what
    experiential conclusions this presentation supports.]}

We present an explanation of the algorithms relevant to MRFy, because there 
exists a natural fit between the mathematics behind the algorithms and the 
Haskell implementation of those algorithms.
If the reader understands the 
algorithms, he or she is more likely to appreciate the benefits of implementing 
them in Haskell.
We will first present the more traditional algorithm used in 
hidden Markov models, followed by our variation on it and the stochastic search 
approach.
We conclude that implementations of mathematically-intensive 
algorithms such as these lend themselves to the Haskell language, particularly 
when the programmer needs a flexible implementation that simplifies 
experimenting with variants of the algorithm.


% As a brief refresher in the standard dogma of genetics, recall that genes 
% (strands of deoxyribonucleic acid (DNA), which are sequences of the nucleotides 
% adenosine, cytosine, guanine, and thymine, represented by the letters A, C, G, 
% and T) are transcribed into ribonucleic acid (RNA). Some RNA is then translated 
% into the 20 naturally-occuring amino acids, which form peptide chains -- 
% proteins -- which fold into complex structures in the cell. These proteins are 
% cellular machinery, performing the varied functions an organism needs to 
% survive. 

% However, determining what newly sequenced genes actually do -- when they become 
% proteins, what structures those proteins fold into, and what functions they 
% perform -- has not kept pace. Determining the atomic coordinates of a newly 
% discovered protein may require as much as two graduate-student-years in a lab! 
% Fortunately, determining the full structure of a protein is not always 
% necessary. Often, we can identify well-known proteins of similar sequence or 
% structure to a new protein, and thus make educated guesses as to the structure 
% and function of the new protein.
% 
% The problem of taking a newly found protein sequence about which only the 
% sequence is known, and determining what existing proteins of known structure 
% and function most closely resemble that new protein, is known as homology 
% detection. When proteins of similar sequence can be found, this problem is 
% largely solved. To solve the homology detection problem, computational 
% biologists have developed fast, approximate methods for determining the 
% structure and function of proteins based on their sequence. The most popular 
% software for homology detection is called HMMER, which uses a hidden Markov 
% model to capture evolutionary change.

% However, when there are no known proteins of similar sequence, only 
% evolutionarily distant proteins that may share similar structure, this problem 
% is called remote homology detection. Hidden Markov model approaches begin to 
% fail as sequence similarity falls off. Recently, Markov random field 
% approaches, which capture non-local interactions in the protein structure, have 
% been shown to perform well. However, these approaches face the challenge of 
% increased computational complexity; the SMURF program exhibits exponential time 
% complexity, while SMURFLite (work by one of the authors) bounds the exponent by 
% simplifying the Markov random field's dependency graph. MRFy is a 
% stochastic search approach to Markov random fields.


\remark{Dropped the section on ``our background,'' but the information
  should reappear when and where we discuss barriers to entry in more
  detail. ---NR}


\section{The Software}

%% \subsection{The Viterbi Algorithm}

Homology-detection software most often used in one of two ways:
to test a hypothesis about 
the function of a single, newly discovered protein, or 
to compare every protein in a genome against a library of known protein 
structures.
\remark{Drop this sentence? This latter case demands particularly high-performance software.}
In~both cases, the software is ``trained'' \remark{do we want to talk ``Training''?}
on a group of proteins that share function and structure.
These proteins are identified by a biologist, who puts
their amino-acid sequences into a correspondence, also called an
\emph{alignment}.
\remark{What does the alignment signify?  Biological significance?
  Computational significance?}
An~alignment may be represented as a matrix
(Figure~\ref{alignment}) 
in which each row corresponds to the amino-acid sequence of a protein,
and each column groups amino acids that play similar roles in
different proteins.
A~column
may contain gaps, which indicate
indicating certain protein have no amino acid participating in that
position.
\remark{The word ``position'' hasn't been defined, and can we get away
  without ``residue''?  (If~we need ``residue,'' let's give it a
  proper introduction.}
If~a column contains just one or two gaps, the most likely explanation
is that in one or two proteins, mutations in the underlying DNA
deleted an amino acid from that position (\figref{alignment}).
If~a column contains \emph{mostly} gaps, 
the most likely explanation
is that in one or two proteins, mutations in the underlying DNA
\emph{inserted} an amino acid in that position (\figref{alignment}).

The protein alignment is used to train a \emph{hidden-Markov model}.
\remark{I've never seen this hypenated, in ANY literature.}
A~hidden-Markov model is a probabilistic finite-state machine which 
can assign a probability to any amino-acid sequence under test.
\remark{What is the biological or computational meaning of the
  probability?}
Proteins whose sequences have higher probabilities are more likely to
be structurally related to the proteins in the alignment.
\remark{Should we say ``homologous'' rather than  ``structurally
  related''?
  If~so, where should this jargon be introduced?}
The states of the hidden-Markov model are determined by the alignment:
\remark{I've tried for lots of parallel structure.
Somebody let me know if any of this is right.  ---NR}
\begin{itemize}
\item
For a given column~$C$ of the alignment, the hidden-Markov model has a
\emph{match state}.
The match state determines the probability that the sequence under
test has a particular amino acid in column~$C$.
The match state assigns high probability to the amino acids that
appear in column~$C$, and a low but nonzero probability to other amino
acids.
\item
For every column of the alignment, the hidden-Markov model has a
\emph{deletion state}.
The deletion state determines the probability that the sequence under
test has \emph{no} amino acid  in column~$C$, i.e., that it has a gap.
The deletion state has high probability if there are lots of gaps in
column~$C$,
and low probability if there are few gaps in column~$C$.
\item
Between every adjacent pair of columns in the alignment,\remark{This isn't right}
the hidden-Markov model has an
\emph{insertion state}.\remark{concept of consensus columns is what's missing}
The insertion state determines the probability that the sequence under
test has an ``extra'' amino acid that does not correspond to any
column in the alignment.
The insertion state assigns high probability to \emph{[some amino
    acids]}, and low probability to \emph{[other amino acids]}.
\end{itemize}
%%  A position that has residues in a majority of the sequences becomes 
%%  a \textit{match} state in the hidden Markov model; this position may see a 
%%  variety of amino acids due to evolutionary substitution, but it still 
%%  corresponds to a position that a homologous protein should most likely fill.
The hidden-Markov model also has distinguished \emph{begin} and \emph{end} states.
\figref{plan7} shows a hidden-Markov model derived from an alignment
with just four columns.
\emph{[I'm sure I don't have the proper explanation of the probabilities.]}
\remark{LOST: Each edge in the 
graph has a weight which is the probability of the associated state transition.}
Each column in the original alignment corresponds to a \emph{node},
which is a triplet holding an insertion state, a deletion state, and a
match state.
What looks like an 
intimidating geometry is thus completely explained by the fact that we have 
insertions, deletions, and substitutions due to evolution.
\remark{The evolution seems like a distraction to me here, and we hit
  it above.  Can we simply drop this sentence?}
\emph{[HOW ABOUT SOME HASKELL TYPE DEFINITIONS AT OR NEAR THIS POINT?
    I THINK WE'RE READY.]}


\begin{figure}
\ifpdfmadness
\centerline{\includegraphics[width=6cm]{alignment.pdf}} 
\else
\centerline{\includegraphics[width=6cm]{alignment.eps}} 
\fi

\emph{[Not sure we need this text: 
Protein sequences are put into alignment based on structural or 
sequence similarities.
Each column of the alignment corresponds to a node of 
the hidden Markov model; columns with many gaps will induce highly probable 
insertion states in the HMM, while columns with fewer gaps will induce 
more-probable deletion and match states.
Columns with strong consensus towards 
a particular residue will induce match states with high emission probability 
for that residue.]}


\caption{A~structural alignment of four proteins}
\label{alignment} 
\end{figure}


\begin{figure} 
\ifpdfmadness
\centerline{\includegraphics[width=8cm]{Plan7.pdf}} 
\else
\centerline{\includegraphics[width=8cm]{Plan7.eps}} 
\fi

This directed graph represents the states of a
hidden-Markov model in the ``Plan7'' format defined by the HMMER
software package.
%% Only 7 of the possible 9 state transitions are allowed, which 
%% simplifies the dynamic programming.
Each insertion state~($I$) represents a potential mutation that 
inserts an amino acid;
each deletion state~($D$) represents a potental mutation that
deletes an amino acid;
and each match state~($M$) represents either
an unmutated amino acid or a potential mutation that
substitutes one amino acid for another.
A~match state or an insertion state includes a table that assigns a
nonzero probability to each amino acid.
(The figure shows tables for the first two states only.)
A~triplet of an insertion state, deletion state, 
and match state formas a \emph{node}, which corresponds to a column in
the alignment used to train the model.
(Each node is shown in a dashed oval.)

\caption{A hidden-Markov model trained from a four-column alignment}

\label{plan7} \end{figure}


A hidden-Markov model like that shown in \figref{plan7} can be used to
determine the likelihood that a new protein shares structure and
evolutionary history with the proteins used to train the model.
(The new protein's amino-acid sequence is known, but its structure and
function are unknown.)
Such a likelihood can be computed by
a~widely used software package called HMMER (``hammer'').
HMMER computes the optimal \textit{parse}, or 
alignment, of a protein sequence onto the directed graph of the hidden-Markov 
model.
A parse maps each amino acid of the sequence under test onto a state
of the model; equivalently, a parse is a \textit{path} through the model.
Each edge is weighted with a probability, and each emitting 
state (match or insertion) maps each amino acid to a probability,
and the 
probability of a given path is the product of these individual probabilities.
The most likely path, and its associated probability, are computed 
using an algorithm developed by~\citet{Viterbi:1967}.


The Viterbi algorithm comprises a set of recurrence 
relations (see Equation \ref{viterbi_eqn}) that seek to maximize the 
probability of an observed sequence (in this case, of amino acid ``letters'') 
being emitted by a particular finite-state automaton (the hidden Markov model).

To be efficient, the Viterbi 
algorithm relies on dynamic programming, or memoization, and the resulting 
asymptotic complexity is $O(|M|\times|N|)$, when $M$ is the sequence of 
distinct states (nodes in the automaton's graph) and $N$ is the sequence of 
letters in the protein sequence.
In our particular problem domain, both $M$ and 
$N$ routinely contain several hundred to a few thousand elements.
The types of 
these states and probabilities are provided in Figure~\ref{viterbi_types}.

%% break the viterbi types up and insert into the Plan7 explanation.
%% go over this with NR; I can't make it flow.

\begin{figure}
\lstinputlisting[label=viterbi_types,caption=Viterbi Types]{viterbi_types.hs}
\end{figure}


The textbook Viterbi recurrence relations refer to logs of probabilities, so 
that the terms may be added rather than multiplying raw probabilities.
For 
reasons including floating point underflow, the HMMER software (with which we 
maintain file format compatibility) stores all probabilities in a trained HMM 
file as negative natural logs.
Thus, the Viterbi recurrence relations are 
simplified from the form in \ref{viterbi_eqn} to that in \ref{viterbi_log_eqn}, 
and because they are \textit{negative} logs, the problem transforms from 
maximization to minimization.


\begin{eqnarray}    
V_{j}^{M}(i) = \log\frac{e_{M_{j}}(x_{i})}{q_{x_{i}}} + max \left\{
\begin{array}{l l}
V_{j-1}^{M}(i - 1) + \log a_{M_{j-1}M_{j}},\\
V_{j-1}^{I}(i - 1) + \log a_{I_{j-1}M_{j}},\\
V_{j-1}^{D}(i - 1) + \log a_{D_{j-1}M_{j}}\\
\end{array} \right.\nonumber\\
V_{j}^{I}(i) = \log\frac{e_{I_{j}}(x_{i})}{q_{x_{i}}} + max \left\{
\begin{array}{l l}
V_{j-1}^{M}(i - 1) + \log a_{M_{j-1}I_{j}},\\
V_{j-1}^{I}(i - 1) + \log a_{I_{j-1}I_{j}},\\
\end{array} \right.\nonumber\\
V_{j}^{D}(i) = max \left\{
\begin{array}{l l}
V_{j-1}^{M}(i - 1) + \log a_{M_{j-1}D_{j}},\\
V_{j-1}^{D}(i - 1) + \log a_{D_{j-1}D_{j}}\\
\end{array} \right.\nonumber\\
\end{eqnarray}\label{viterbi_eqn}

\begin{eqnarray}    
V_{j}^{\prime M}(i) = e^{\prime}_{M_{j}}(x_{i}) + min \left\{
\begin{array}{l l}
V_{j-1}^{\prime M}(i - 1) + a^{\prime}_{M_{j-1}M_{j}},\\
V_{j-1}^{\prime I}(i - 1) + a^{\prime}_{I_{j-1}M_{j}},\\
V_{j-1}^{\prime D}(i - 1) + a^{\prime}_{D_{j-1}M_{j}}\\
\end{array} \right.\nonumber\\
V_{j}^{\prime I}(i) = e^{\prime}_{I_{j}}(x_{i}) + min \left\{
\begin{array}{l l}
V_{j-1}^{\prime M}(i - 1) + a^{\prime}_{M_{j-1}I_{j}},\\
V_{j-1}^{\prime I}(i - 1) + a^{\prime}_{I_{j-1}I_{j}},\\
\end{array} \right.\nonumber\\
V_{j}^{\prime D}(i) = min \left\{
\begin{array}{l l}
V_{j-1}^{\prime M}(i - 1) + a^{\prime}_{M_{j-1}D_{j}},\\
V_{j-1}^{\prime D}(i - 1) + a^{\prime}_{D_{j-1}D_{j}}\\
\end{array} \right.\nonumber\\
\text{where } a^{\prime}_{s} = - \log a_{s} \nonumber \\
\text{and } e^{\prime}_{s,x} = - \log\frac{e_{s,x}}{q_{x}} \nonumber
\end{eqnarray}\label{viterbi_log_eqn}


We implemented the Viterbi algorithm in two distinct ways.
We first wrote a 
bottom-up dynamic-programming version, which began with the first node of the 
hidden Markov model and first observation and built up the three-dimensional 
array of possible paths through the model.
The array containing the paths 
served as the memoization table.
We next wrote a top-down dynamic-programming 
version, which began with the last node of the model and the last observation, 
and applied structural recursion down to the base cases involving the first 
node or first observation (or both).
In this second approach, we used the 
\texttt{Data.Memocombinators} library to provide the memoization container 
(again, a three-dimensional array).
We determined that the run-time 
performances of both approaches were virtually indistinguishable, and we also 
noted that the top-down version quite faithfully resembled the recurrence 
relations found in a textbook~\cite{durbin} or the HMMER literature~\cite{eddy} 
(see Equation \ref{viterbi_eqn}).
We chose to keep the top-down version, which 
later proved to be wise when we had to track down a bug in one of our base 
cases.
This bug was observed as a missing state in the alignment our program 
displayed, and we quickly discovered that one of our base cases was missing 
this state: it was returning an empty list as the base of the path, rather than 
a list containing a single node.
In this, we were grateful for the resemblance 
between the mathematical description of the algorithm and the top-down 
dynamic-programming approach in Haskell, which resulted in perspicuous code.

\begin{figure*}
\lstinputlisting[label=viterbi,caption=Viterbi]{viterbi_simple.hs}
\end{figure*}


\subsection{MRFy Implementation}

HMMER simply implements the Viterbi algorithm for computing the optimal parse 
of a protein sequence onto a hidden Markov model.
Thus, the graphical model of 
a HMMER hidden Markov model is exactly that shown in Figure~\ref{plan7}.
In 
contrast, MRFy builds on the approach of SMURF~\cite{smurf} and 
SMURFLite~\cite{smurflite}, our previous work.
Like SMURF and SMURFLite, MRFy 
pays attention to longer-range structural interactions than a simple hidden 
Markov model can understand.
In particular, these approaches ``nail down'' 
certain subsequences of the model, corresponding to the non-local hydrogen-bond 
interactions between beta strands.
Beta strands are regions of protein 
structure that ``stick'' to one another.
Because of their pairwise 
interactions, they are highly conserved over the course of evolution.
Moreover, 
again because of these pairwise interactions, a change in one amino acid in a 
beta strand may require a change in a corresponding, hydrogen-bonded residue in 
another beta strand if the protein is to remain stable.
The architecture of the 
resulting graphical model, derived from the HMMER ``Plan7'' architecture, is 
illustrated in Figure~\ref{mrf}.

These non-local interactions cause what had been a simple hidden Markov model 
to become a Markov random field.
SMURF and SMURFLite attempted to compute the 
optimal parse of a protein sequence onto the directed graph representing a 
Markov random field using multidimensional dynamic programming.
SMURF's 
computational complexity quickly exploded with complex protein structures, 
while SMURFLite limited its computational complexity by also limiting the 
complexity of protein structure it would understand.
The algorithm underlying 
MRFy grew out of an understanding of these limitations.


Rather than attempt to compute exactly the optimal parse, MRFy treats the beta 
strands as ``beads'' which can be slid along a string.
In this analogy, the 
string is the sequence of amino acids of the query protein.
A given placement 
of these beads corresponds to an assignment of particular amino acid residues 
being hydrogen bonded to other particular amino acid residues, and thus MRFy 
computes a probability based on observed frequencies of residues hydrogen 
bonding in beta strands~\cite{betawrappro}.
In between these beta strands, the 
Markov random field is in fact a traditional hidden Markov model, whose optimal 
solution can be easily computed via the Viterbi algorithm.
MRFy performs a 
stochastic search to place these beta strands, based on an objective function 
that combines the probability computed by the Viterbi algorithm with that 
computed from the paired, hydrogen-bonded residues in the beta strands.
If a 
structure has many beta strands, then it also has many miniature hidden Markov 
models which can be solved via the Viterbi algorithm.


\begin{figure}[h!] 
\ifpdfmadness
\centerline{\includegraphics[width=8cm]{mrf_interleave_diagram.pdf}} 
\else
\centerline{\includegraphics[width=8cm]{mrf_interleave_diagram.eps}} 
\fi
\caption{The directed graph representing the states of a MRFy Markov random 
field finite state automaton.
Shaded nodes represent beta-strand positions which have been fixed into match states, 
while the dashed edges represent the long-range dependencies due to hydrogen 
bonding.
Note that in between the beta-strand regions are normal HMMs.}\label{mrf} \end{figure}


MRFy contains two distinct algorithms: the classical Viterbi 
dynamic-programming algorithm for hidden Markov models, and a stochastic search 
algorithm for placing beta-strand positions.
This stochastic search algorithm 
implements several search heuristics: random-mutation hill climbing, simulated 
annealing, multistart simulated annealing, and a genetic algorithm.


Just as the Viterbi algorithm is one major component of MRFy, stochastic search 
is the other.
In designing MRFy, we did not know in advance what sort of 
stochastic search technique would converge the fastest, or give us the best 
alignments with the fastest performance.
We decided to implement several 
different search strategies so that we could experiment with all of them.
We 
found this to be easy and pleasant thanks to Haskell's support of higher order 
functions.

We wrote a basic search function in only twenty-two lines of code; as one of 
its parameters we provide a \texttt{SearchStrategy} data type, which we use to 
define specific search techniques.
We require a \texttt{SearchStrategy} to 
define four functions: \texttt{mutate}, \texttt{accept}, \texttt{terminate}, 
and \texttt{initialize}; we use the Haskell module system to allow sharing of 
some of these functions between different search strategies.
For example, both 
\textit{random mutation hill climb} and \textit{simulated annealing} use the 
same initialize and mutate functions, but very different accept functions.
In 
all, we implemented four search strategies: \textit{random mutation hill 
climb}, \textit{simulated annealing}, \textit{multistart simulated annealing}, 
and \textit{genetic algorithm}.
We found the implementation of the search 
function and search strategies to be pleasant, and the resulting code to be 
simple and elegant.
In an imperative language with object orientation like Ruby 
or Python, we likely would have used a class hierarchy to accomplish this.
We 
feel that this would have required more code, because object orientation is 
meant for encapsulating data as well as functions on those data; here we only 
wanted to encapsulate functions, as the data did not change.


We note that \texttt{seed} in \texttt{search} comes from a monadic random 
number generator in Main, but we wished for search itself to be pure at this 
level.
One advantage is that this allows for testing of the \texttt{search} 
function with no randomness at all: if we pass in a known number as the seed, 
behavior is entirely deterministic.


\begin{figure*}
\lstinputlisting[label=search,caption=Search]{search_simple.hs}
\end{figure*}

In MRFy, the Viterbi function is called several times (once for each 
non-beta-strand region) for each search generation.
Furthermore, in a 
population-based search such as \textit{multistart simulated annealing}, the 
scoring function (which itself calls Viterbi) is called many times per 
generation.
Beyond traditional profiling, we found parallelization to be an 
obvious way to improve runtime performance.
We found this to be trivial thanks 
to the \texttt{Control.Parallel} library.
We pass the Viterbi function to a map over all 
non-beta-strand regions of the model and query sequence, and these regions are 
independent of one another.
Similarly, we pass the scoring function to a map 
over the population of potential solutions, which are also independent.
We 
merely substituted \texttt{parmap rseq} for map.
MRFy now efficiently uses all six 
cores on an AMD workstation for a non-population-based simulated annealing 
search, and all forty-eight cores on a compute server for population-based 
searches.


\subsection{Ugly code}


Despite the beauty of some of the code we were able to write, we found several 
instances where the easiest path seemed to be to write ugly code.
The first of 
these relates to the otherwise elegant Viterbi function.
Our stochastic search 
approach calls the Viterbi function a tremendous number of times, and most of 
the program's runtime is spent in this function.
However, the Viterbi algorithm 
retains not just a score (the probability of the sequence given a state path) 
but also the state path itself.
We implemented this state path as a list of 
\texttt{Int}, and every step in the recursion conses a new element onto the 
list.
However, at each step of the stochastic search, we discard the state 
path; we are searching to optimize score only.
We only need the state path at 
the termination of the search, when we wish the program to output an alignment 
of the query sequence to the Markov random field model.
We found that 
optimizing the Viterbi function by removing this state path entirely improved 
runtime performance by nearly 50\%.
However, we still needed the Viterbi 
function to provide a state path when we called it after search finally 
terminated.
Thus, we wondered how to implement the Viterbi function so that a 
state path was optional.

In order to determine if this optimization was even worthwhile, we first wrote 
a quick and dirty approach that is shameful in any language, but easy: we 
duplicated code.
We copied \texttt{viterbi} to produce a \texttt{viterbiF} 
which did not retain state path.
Next, however, we wondered how to avoid code 
duplication.
Clearly, littering the code with conditionals would produce code 
that was ugly as well as less efficient.
We knew how we would solve this 
problem in other languages; we would have used macros in Lisp, or 
metaprogramming via \texttt{eval} in Ruby, to produce two versions of the 
function from one prototype.
We struggled, however, to solve this problem 
idiomatically in Haskell.

Rather than attempt to use \texttt{TemplateHaskell} to generate two versions of 
the Viterbi function, we wrote a higher-order function type 
\lstinline!type ScorePathCons a = a -> [a] -> [a]! and two functions conforming to that type.

Then, depending on whether \texttt{viterbi} was called with \texttt{consPath} 
or \texttt{consNoPath} as a parameter, the path would or would not be 
allocated.
We found this higher-order function approach to be simple in code 
and in concept, and it imposes negligible run-time overhead as compared to 
having two distinct functions.

\lstinputlisting[label=scorepathcons,caption=ScorePathCons]{scorepathcons.hs}

Another source of ugliness resulted from adding cost-center annotations when we 
wished to profile our code.
GHC will happily add cost centers to top-level 
functions, but much of our program relies on sub-functions.
We found that 
manually adding \texttt{\{-SCC -\}} annotations to dozens of guard clauses and 
sub-functions harmed the readability of our program, such that we felt 
compelled to remove all such annotations as soon as we could.
In our prior 
experience with profiling tools such as kcachegrind, simply compiling with 
debug symbols -- rather than manually annotating many lines of code -- allowed 
the profiling tools to provide us enough information.
Without these 
annotations, both call-site and allocation profiles are relatively useless in 
GHC.
Perhaps a compile-time flag that would auto-annotate every equation -- 
even if the resulting cost center is simply named with a line number -- would 
make this task easier.

The final source of ugliness in our code came from attempts at debugging 
run-time errors.
MRFy relies on a source of input data -- the paired beta 
strands in the Markov random field -- which is represented in an awkward manner 
by the original SMURF program, and which is outside our control.
Rather than 
represent beta strands directly, SMURF represents pairings of residues, leaving 
us to infer and reconstruct beta strands.
This task was difficult due to 
unclear invariants around overlapping, doubly-paired beta strands (and it would 
have been difficult in any programming language; the difficulty was one of 
representation).
However, the debugging tools in GHC caused us some 
difficulties in determining the sources of run-time errors.
We used 
\texttt{trace} extensively, but this style of ``\texttt{printf} debugging'' 
littered our code.
Being initially unaware of the backtrace feature of the GHC 
profiler, we wrote wrappers around \texttt{Vector.slice} and \texttt{Vector.!} 
which themselves called trace, which at least kept our ``\texttt{printf} 
debugging'' less cluttered.
Nonetheless, we struggled with these runtime errors 
due to an awkward representation, and wished for debugging tools such as we 
might find in gdb, which would allow us to examine the stack when a backtrace 
occurs.
 
 
\section{Our previous code base compared}

Our previous computational biology projects include an enhancement to a protein 
structural aligner that previously came from our group, Matt~\cite{matt}.

Matt's code base contains approximately 12,000 lines of C++ code, and relies on 
data structures such as mutable oct-trees, vectors, and arrays, with 
significant amounts of clever pointer arithmetic.
Our enhancement involved 
modifying Matt to incorporate sequence information along with the structural 
aligner.
While we initially estimated this would be a three-month project, it 
took most of a year because the mutable data structures were difficult to 
repurpose, and the pointer arithmetic was too clever for its own good: since 
invariants were not clear, nearly every change resulted in new segfaults.
One 
enhancement, partial alignments (essentially a cosmetic manipulation of the 
output, but requiring deep information about some of the data structures) was 
deemed infeasible.
We believe we could have completed a ground-up rewrite in 
Haskell, with most of the run-time performance, in fewer than nine months.


In general, the failure modes we encounter most in C++ programming are 
segmentation faults, memory errors (due to pointer arithmetic and uninitialized 
memory) and heap exhaustion.
In contrast, the failure modes we most commonly 
see in Haskell code are type-check failures, or at worst, runtime errors around 
bounds checking of arrays and vectors.
While bounds checking failures have 
proved difficult to debug, they are still less so than C++ memory errors.


Haskell makes it difficult to write code in haste, because Haskell militates 
towards careful type-checking and clearly thought-out function contracts.

Unlike dynamic languages such as Ruby, and unlike C++, Haskell punishes the 
programmer who attempts to write code quickly with the goal of later making it 
work correctly.
We have found MRFy to be easier to enhance and maintain than 
our existing C++ code bases.
Despite the learning curve of Haskell, we 
implemented most of MRFy in roughly three months of part-time work.

 
\section{State of the practice}

\subsection{Our group then}
 - Modifying Formatt took nearly a year
 - mutable data structures hard to repurpose
 - one enhancement deemed infeasible
 - C++ failures most commonly segfaults, memory errors, heap exhaustion

\subsection{Our group now}
 - Higher order functions militate towards more flexibility
 - Haskell failures are usually typecheck failures; at worst runtime errors around bounds checking
 - MRFy about 3 months of part-time work

\subsection{Computational biology at large}
 - some good software, like HMMER

\section{What we think about it}

\subsection{Our Background}

\remark{I moved this section from the Intro as per NR's suggestion. ---AG}
All three authors are members of the Tufts University Department of Computer
Science.
Noah Daniels has taken a graduate seminar in advanced functional
programming, which included some Haskell, and spent ten years in industry as a
professional programmer in languages such as Ruby, C, and C++.
Andrew Gallant has taken a programming languages course heavily focused on 
functional programming, but with no Haskell.
Norman Ramsey is the local expert, but he wrote no code for this project.

\subsection{Barriers}
\remark{I feel like a lot of this is a summary of some of the problems we've
  discussed previously in the paper. Is that okay? A summary may be worth it. 
---AG}

As inexperienced functional programmers, developing a complex piece of
software in Haskell posed several barriers.
\remark{NOAH: You did the profiling, could you make sure what I've said here
is accurate and/or relevant? I didn't know what else to say. ---AG}
The first of which was profiling with GHC.
We previously mentioned that profiling with GHC produced ugly code with
cost center annotations scattered throughout our code, but we also found it
very difficult to profile code at all.
In particular, profiling required that all installed libraries be re-compiled
with profiling support---which was a process that we could not replicate
consistently.

Another barrier we faced was how to test our code using QuickCheck.
Indeed, the barrier was so formidable in our case that we still have not been 
able to use it.
The problems we faced specifically were two-fold: How do we start testing
a complex module? and How can we use QuickCheck to generate random inputs with 
complex constraints?

Finally, one last barrier we faced was debugging runtime errors.
Runtime errors in the form of out-of-bounds indexing were particularly 
difficult to track down because they were not accompanied by a stack trace.
(While profiling with GHC could help remedy this, profiling was itself a 
barrier, as previous mentioned.)
We had to resort to polluting our code with cals to the \texttt{trace} function 
in order to track down where the out-of-bounds errors were occurring.

% - profiling 
% - debugging 
 % [ talk about profiling and debugging problems where?] 
% [ something about quickcheck: how do we know when we have enough QC props? How do we start testing a complex module? ] 

\subsection{What you must know to succeed}
- Ecosystem is loaded with tools that look promising but either are no longer maintained or are not ready for prime time (Pads, Backtrace)

\subsection{Evaluation}
- What does the local expert do? Do you have to have one?

- To the FP community: Those who want us to abandon our old tools for debugging imperative code have not provided a clear path to follow (both intellectually, and via software tools).

- No obvious body of knowledge on code improvement.

- The ecosystem contains tools that look promising and work great: Data.Memocombinators, Quickcheck, Parallel Strategies.
How to tell what's good? How did we discover what's good?
- Profiler can produce backtraces, but this was hard to discover.

% \appendix
% \section{Appendix Title}
% 
% This is the text of the appendix, if you need one.

\acks

We thank Lenore Cowen, Kathleen Fisher, Benjamin Hescott, Bradford Larsen, and Nathan Ricci.


\bibliographystyle{plainnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}

\end{document}
